---
layout:     post
title:      muduo阅读笔记之第9章
subtitle:   分布式系统工程实践
date:       2018-04-01
author:     BY
header-img: img/post-bg-swift.jpg
catalog: true
tags:
    - muduo
---


### 在编程模型方面，分布式对象已被淘汰
   这种模型的本质难点在于容错语义。</br>
   依次RPC超时，调用方无法区分：是网络故障还是对方机器崩溃；是去的路上还是回的路上；对方有没有收到请求，能不能重试；你能信得过他内置的容错与对象迁移机制吗？</br>
   google好文《introduction to distributed system design》，点睛之笔：分布式系统设计，是design for failure。</br>
### 大规模分布式系统处于技术浪潮的前期
   尚未形成一套完整的方法论，有的只是一些零散的经验。</br>
   没有一个分布式系统职位的工人的好的面试题。如果考察c++，虚析构是必问。如果考察多线程，死锁和race condition必问。考察分布式程序员呢?</br>
### 分布式的本质困难
   分布式的本质困难是存在partial failure。</br>
### 分布式系统是个险恶的问题
   一、实现一个系统前，无法预测那个技术方案行的通。（先定义，思考后再迭代解决）</br>
       假设有一个缩略图（Thumbnailer）服务，提供数码照片按比例压缩（缩略图相当耗时），不影响web服务器性能，把生成缩略图放到单独服务器中。如何做负载均衡？</br>
           方案1：每台web服务器只和一台Thumbnailer打交道，通过web本身的负载均衡分散到多个Thumbnailer上。</br>
           问题：负载是非均质的，具有突发性，如果某个用户通过一个web服务器上传一堆照片，会出现一个Thumbnailer满负荷，其他都闲着。</br>
           方案2：每个web服务器都和每个Thumbnailer打交道（网状）。</br>
                方法1（开环）：web服务器走马灯式选择Thumbnailer。导致负载也走马灯移动。</br>
                方法2（开环）：web服务器随机选择Thumbnailer。web服务器几乎同时启动，初始化种子相同，与方法1相同。</br>
                方法3（闭环）：让Thumbnailer向web定期汇报负载。每个周期要发送M*N条消息，伸缩性不佳。</br>
                方法4（闭环）：通过集中的负载均衡器来收集并分发负载。造成单点故障（Single Point of Failure，SPoF）</br>
                方案5（客户端视角）：响应客户端（web服务器）的请求的快慢直接反映了服务端（Thumbnailer）负载。客户端把服务器看成循环队列，选择服务器从上次调用的服务端的下一个位置开始，找出负载最轻的（发出而尚未结束的请求个数）服务端。</br>
                分布式系统中保持全局数据一致性是个老大难的问题。</br>
    二、时间和事件顺序违反直觉，因为消息传递的延时是不固定的。</br>
### 分布式可靠性浅说
    提高可靠性：一、提高冗余；二、降低重建数据的时间。
    可靠性：不丢数的概率。可用性，数据服务随时被访问的概率。
    
- 分布式系统的软件不要求7*24可靠
         硬件故障不可避免（硬盘的年故障略再3%到8%。SATA硬盘，每读12TB数据就会有数据读不出来。100台服务器组成的分布式系统，每个月出现依次硬件故障的可能性略大于50%。）软件可靠性略高于硬件及操作系统即可。</br>
        单机系统，程序应尽量可靠（7*24），分布式系统，可随时重启（一两个月版本更新也需要重启程序）。</br>
- 能随时重启进程”作为程序设计目标
        重启时间尽量短，尽量做到不中断服务。</br>
        不要使用夸进程的mutex或semaphone，不要使用共享内存，如果进程意外终止的话，无法清理资源，无法解锁。也不使用父子进程共享文件描述符来通信（pipe），父进程死了，pipe无法重建。</br>
        升级：1、热升级。2、迁移到新版本的服务进程，然后停止旧版本进程。</br>
	
在运行期热替换dll通常是走火入魔的标志，真的需要在运行时替换程序逻辑的话，可以用嵌入脚本语言，把代码替换为数据

### 心跳协议设计
        tcp keepalive替代不了心跳，因为，进程死锁或阻塞，操作系统也会正常收发tcp keepalive。
        心跳超时，通常可取timeout=2Tc，（Tc为发送端发送心跳周期）
        注意，闰秒时会跳变。
        Google有一篇blog。
        心跳协议的关键点：
                1、要在工作线程发送，不要单独起一个“心跳线程”。（应该在工作线程注册定时器回调）
                2、与业务消息同一个连接。
		
### 分布式进程标识
   使用ip:port:start_time:pid作为进程标识，可以追溯历史，了解分布式系统中，哪个进程是否重启。start_time是64位整数，表示UTC时间。pid，分配空间很小，可能轮回，不能使用ip:port:pid。如果进程本身没有port，通常进程中有个维护探查的通道服务，可以使用那个port。</br>
   tcp中的seq（SYN中）也是为了区分相同地址（localip,port:remoteIp,port）本次连接和上次连接的干扰，相当于start_time。</br>

### 易于维护的分布式程序
   分布式系统中的每个进程，都应该应该提供一个管理接口，维修探查通道，可以查看进程全部状态。可以汇总查看所有进程的运行情况。</br>
   使用http，便利。</br>
	
	1监视服务进程的运行境况:
	2到目前为止累计接受了多少个tcp
	3当前有多少个活动连接
	4每个活动连接的用途是什么
	5一共响应了多少次请求
	6每次请求的平均输入输出数据长度是多少个字节
	7每次请求的平均响应时间是多少毫秒
	8进程平均有多少个活动请求
	9并发请求的峰值是多少，出现在什么时候
	10某个连接上平均有多少个活动请求
	11进程中xxxrequest对象有多少分实体
	12进程中打开了多少个数据库连接，每个连接的存活时间是多少
	13程序中有个hashmap，保存了当前的活动请求，我想把他打出来
	14某个请求似乎卡在某个步骤，我想打印进程中该请求的状态

### 为系统演化扩展做做准备
   系统会进行多次演化升级，需要一开始就为将来考虑。</br>
   注意，通信双方可能不会同时升级。客户端服务器可能也不是同一团队，同一语言。因此需要保证协议功能的兼容性。</br>
   - 不好的做法：</br>
       1、协议中有版本号，根据版本号分发。容易留下垃圾代码，背着历史包袱。</br>
       2、使用C struct 和bit field。不夸语言，通常需要服务端和客户端一起升级。</br>
   - 正确做法：</br>
       使用中间语言描述消息格式。如果用文本格式，可以考虑JSON或XML。如果用二进制格式，可以考虑Google ProtoBuffer.(每个field有终生不变的id，可看作数据块类型，可以客户端先升级，服务端可以跳过不认识的field字段。)。不要改变存在field的数字标号，不要增加或删除required field。可参考PNG文件（二进制文件格式）。</br>
分布式程序的自动化回归测试</br>
        自动化测试作用：把程序已实现的features以case形式固定下来，将来任何代码改动如果破坏了现有功能需求，就会触发测试failure。</br>
        分布式系统中，class和function级别的单元测试对整个系统的帮助不大，单元测试对单个程序的质量有帮助，对分布式系统效果较小。</br>
        单元测试依赖被测试代码，被测试代码重构，单元测试就失效。</br>
        分布式测试要点，是测试进程的交互。测试多进程协作的场景才算测到点上了。开发阶段，不会因为其他组负责的进程进度延误，影响自己功能的开发。</br>
        千兆以太网，吞吐量不大于125MB/s。这个吞吐量比起现在cpu速度和内存带宽简直晓得可怜。因此IO饱和，用啥语言写服务端程序无所谓。例如c++可能用了15%cpu，java用了30%cpu。</br>
        为被测进程单独编写一个test harness进程，mock(模仿)了与被测进程打交道的全部程序。</br>
        优点：
	
            1、不会对被测程序入侵。
            2、能测试真实环境下的表现。
            3、允许被测程序重大重构，不影响test harness。
            4、能够方便测试断网、超时等failure场景。
            5、被测程序可以换其他语言重写，test harness和测试用例仍然可以用。
	    
- 实现要点：

            1、test case以配置文件方式指定。并且使用版本控制，能够复现任何一个版本的所有行为。
            2、test harness可以只有一个命令行界面，输入run 10,运行第10号test case。
            3、test harness可以表现跟他要mock的程序一样，不需要真的实现逻辑。
	    
### 分布式系统部署、监控与进程管理的几重境界

一个master，兼做name service，需要冗余。</br>
每个节点运行一个slave，定期向master汇报节点资源使用情况，并监视服务意外重启。</br>

- 部署
        master发一条指令，salve从指定地点rsync新的可执行文件，使用md5sum检查文件是否与源文件相同</br>
        配置文件可以使用版本管理工具，checkout</br>
- 升级
        发布新版本严禁覆盖已有的可执行文件，dump可追溯</br>
        master发送升级指令，不用ssh&kill</br>
- 配置
        修改服务的配置文件，需要通知用到他的服务的配置文件。</br>
        master配置文件会指定哪些机器运行哪些服务，命令slave启动相应服务</br>
        服务间的依赖关系，由master配置文件配置。仅一处，降低了犯错的机会。客户端使用的ip:port，由master的name service转换，可解决failover。dns静态，或缓慢变化。名字服务与dns最大区别，可以把新的地址推送给客户端。slave启动服务，会主动更新名字服务，无需自己手动。名字服务不能一台，通常5台服务器提供，需要同步。</br>
- 进程管理和监控
         不光需要有监视，还可以修改。</br>
         slave fork各个服务，当服务崩溃时，slave通过捕获SIGCHLD信号，得知子进程意外退出。比心跳，轮询快。</br>
        
        
